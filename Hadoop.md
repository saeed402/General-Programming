## CAP Theory

**CAP theory**, also known as Brewer's theorem, is a theoretical framework in distributed computing that helps in understanding the trade-offs between three important properties of a distributed system:

- **Consistency (C)**: All nodes in the system see the same data at the same time.
- **Availability (A)**: Every request made to the system receives a response (without guaranteeing that it contains the most recent version of the data).
- **Partition Tolerance (P)**: The system continues to operate even in the presence of network partitions (communication failures between nodes).

According to CAP theory, in a distributed system, you can achieve at most two out of the three properties simultaneously. For example, in the presence of network partitions (P), you must choose between Consistency (C) and Availability (A).

## Hadoop

**Hadoop** is an open-source distributed computing framework designed to process and store large datasets in a distributed and fault-tolerant manner. It is widely used for big data processing and analytics.

## Hadoop Distribution and Version
There are several Hadoop distributions available, including Apache Hadoop, Cloudera CDH, Hortonworks HDP, and others.
## Hadoop components

Hadoop comprises various components, including:

1. **Hadoop Distributed File System (HDFS)**: A distributed file system designed for storing vast amounts of data across multiple machines.

2. **MapReduce**: A programming model and processing engine for distributed data processing.

3. **YARN (Yet Another Resource Negotiator)**: A resource management layer that allocates and manages resources in Hadoop clusters.

4. **Hadoop Common**: A set of shared utilities, libraries, and APIs for Hadoop components.

5. **Hive**: A data warehousing and SQL-like query language for Hadoop.

6. **Pig**: A high-level scripting language for data analysis and processing.

7. **HBase**: A NoSQL database for real-time, distributed data storage.

8. **Spark**: While not a Hadoop component, it's often used in conjunction with Hadoop for data processing.

## MapReduce

**MapReduce** is a programming model and processing framework used in Hadoop for distributed data processing. It consists of two main phases:

- **Map Phase**: In this phase, input data is divided into chunks, and each chunk is processed by a map function that generates a set of key-value pairs.

- **Reduce Phase**: The output of the map phase is sorted and grouped by key, and the reduce function processes these groups of data to produce the final output.

## Mapper vs. Reducer

- **Mapper**: A mapper is responsible for processing the input data and emitting intermediate key-value pairs. It operates in parallel across multiple nodes in a Hadoop cluster.

- **Reducer**: A reducer takes the intermediate key-value pairs generated by mappers, sorts them, and performs aggregation or other operations to produce the final result.

## MapReduce Partitioner

A **MapReduce Partitioner** is responsible for determining which reducer should receive a specific key-value pair produced by the mappers. It helps distribute the data evenly among reducers to optimize the parallel processing of data.

## Combiner

A **Combiner** is an optional component in the MapReduce model. It is used to perform local aggregation of data on the mapper's side before sending it to the reducer. The combiner helps reduce network traffic and improves the efficiency of the MapReduce job by reducing the volume of data transferred between mappers and reducers. Combiners are often used for operations where commutative and associative properties apply, such as summing values.


